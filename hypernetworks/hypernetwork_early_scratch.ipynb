{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Attention\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model\n",
    "#for name, module in model.named_modules():\n",
    "#   print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next to-do:\n",
    "# Write some passable version of a hypernetwork function, that accepts inputs of a context sentence and outputs \n",
    "# some stuff for 1 single layer of the GPT-2 model!\n",
    "\n",
    "# This will be trivial as stated, since we aren't feeding in any inputs to the model as of yet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, writing only the hypernetwork, in a simplified form which does not accept\n",
    "# any input from the target model\n",
    "# The only input is a text string (in this case first sentence of wikipedia)\n",
    "# The output is to be taken directly from layer 8 of the gpt2 model\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "class EditorModelBlind(nn.Module):\n",
    "    # Separating the editor config file, from its base model's configurations\n",
    "    def __init__(self, editor_yaml_file_path='config/editor_blind.yaml'):\n",
    "\n",
    "        super().__init__()\n",
    "        # Load the configuration from the YAML file\n",
    "        with open(editor_yaml_file_path, 'r') as file:\n",
    "            self.editor_config = yaml.safe_load(file)\n",
    "        self.basemodel = GPT2LMHeadModel.from_pretrained('gpt2') #R# replace this with a statement to load from config file\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None, target_cache = None):\n",
    "        result = self.basemodel(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
    "        hidden_states = result.hidden_states\n",
    "        #This cuts the model off at the specified layer in the config\n",
    "        return hidden_states[self.editor_config[\"cut_layer\"]]\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #We have a lot more parameters than we need right now, \n",
    "        #but running it thru torch.compile later will cut down waste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting full hidden state outputs from the model with some input\n",
    "inputtext = \"The quick brown fox jumps over the lazy dog\"\n",
    "editor_ids = tokenizer.encode(inputtext, return_tensors=\"pt\")\n",
    "editor_hidden_states = model(editor_ids, output_hidden_states=True).hidden_states\n",
    "editor_hidden_states[12].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_idx = 0\n",
    "testblock = model.transformer.h[layer_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "testblock.crossattention = GPT2Attention(model.config, is_cross_attention=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to add layernorm. This is slightly different from gpt2 layernorm command, probably due to updates in pytorch\n",
    "testblock.ln_cross_attn = nn.LayerNorm(normalized_shape = 768, eps=model.config.layer_norm_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query_weights = testblock.attn.c_attn.weight[:,:768]\n",
    "original_keys_values = testblock.attn.c_attn.weight[:,768:]\n",
    "original_query_bias = testblock.attn.c_attn.bias[:768]\n",
    "original_keys_values_bias = testblock.attn.c_attn.bias[768:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768, 1536]), torch.Size([768, 1536]))"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testblock.crossattention.c_attn.weight.shape , original_keys_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pytorch_utils.Conv1D"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testblock.crossattention.c_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up the query matrix to be initialized to the same query as cross-attention:\n",
    "#I intend to initialize the key & value matrices to be the same, but then applied to all 12 layers in copies\n",
    "with torch.no_grad():\n",
    "    # Initialize the new layer with these parameters\n",
    "    testblock.crossattention.q_attn.weight = nn.Parameter(original_query_weights)\n",
    "    testblock.crossattention.q_attn.bias =  nn.Parameter(original_query_bias)\n",
    "\n",
    "    testblock.crossattention.c_attn.weight = nn.Parameter(original_keys_values)\n",
    "    testblock.crossattention.c_attn.bias = nn.Parameter(original_keys_values_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[409], line 2\u001b[0m\n",
      "\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Need to add layernorm \u001b[39;00m\n",
      "\u001b[0;32m----> 2\u001b[0m testblock\u001b[38;5;241m.\u001b[39mln_cross_attn \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLayerNorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm_epsilon\u001b[49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'hidden_size'"
     ]
    }
   ],
   "source": [
    "#Need to add layernorm \n",
    "testblock.ln_cross_attn = nn.LayerNorm(hidden_size = 768, eps=config.layer_norm_epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GPT2Block(nn.Module):\n",
      "    def __init__(self, config, layer_idx=None):\n",
      "        super().__init__()\n",
      "        hidden_size = config.hidden_size\n",
      "        inner_dim = config.n_inner if config.n_inner is not None else 4 * hidden_size\n",
      "\n",
      "        self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
      "        self.attn = GPT2Attention(config, layer_idx=layer_idx)\n",
      "        self.ln_2 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
      "\n",
      "        if config.add_cross_attention:\n",
      "            self.crossattention = GPT2Attention(config, is_cross_attention=True, layer_idx=layer_idx)\n",
      "            self.ln_cross_attn = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)\n",
      "\n",
      "        self.mlp = GPT2MLP(inner_dim, config)\n",
      "\n",
      "    def forward(\n",
      "        self,\n",
      "        hidden_states: Optional[Tuple[torch.FloatTensor]],\n",
      "        layer_past: Optional[Tuple[torch.Tensor]] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
      "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = False,\n",
      "        output_attentions: Optional[bool] = False,\n",
      "    ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.ln_1(hidden_states)\n",
      "        attn_outputs = self.attn(\n",
      "            hidden_states,\n",
      "            layer_past=layer_past,\n",
      "            attention_mask=attention_mask,\n",
      "            head_mask=head_mask,\n",
      "            use_cache=use_cache,\n",
      "            output_attentions=output_attentions,\n",
      "        )\n",
      "        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n",
      "        outputs = attn_outputs[1:]\n",
      "        # residual connection\n",
      "        hidden_states = attn_output + residual\n",
      "\n",
      "        if encoder_hidden_states is not None:\n",
      "            # add one self-attention block for cross-attention\n",
      "            if not hasattr(self, \"crossattention\"):\n",
      "                raise ValueError(\n",
      "                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with \"\n",
      "                    \"cross-attention layers by setting `config.add_cross_attention=True`\"\n",
      "                )\n",
      "            residual = hidden_states\n",
      "            hidden_states = self.ln_cross_attn(hidden_states)\n",
      "            cross_attn_outputs = self.crossattention(\n",
      "                hidden_states,\n",
      "                attention_mask=attention_mask,\n",
      "                head_mask=head_mask,\n",
      "                encoder_hidden_states=encoder_hidden_states,\n",
      "                encoder_attention_mask=encoder_attention_mask,\n",
      "                output_attentions=output_attentions,\n",
      "            )\n",
      "            attn_output = cross_attn_outputs[0]\n",
      "            # residual connection\n",
      "            hidden_states = residual + attn_output\n",
      "            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n",
      "\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.ln_2(hidden_states)\n",
      "        feed_forward_hidden_states = self.mlp(hidden_states)\n",
      "        # residual connection\n",
      "        hidden_states = residual + feed_forward_hidden_states\n",
      "\n",
      "        if use_cache:\n",
      "            outputs = (hidden_states,) + outputs\n",
      "        else:\n",
      "            outputs = (hidden_states,) + outputs[1:]\n",
      "\n",
      "        return outputs  # hidden_states, present, (attentions, cross_attentions)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block\n",
    "print(inspect.getsource(GPT2Block))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/michaelsklar/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/michaelsklar/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/model.safetensors\n",
      "All model checkpoint weights were used when initializing GPT2Model.\n",
      "\n",
      "All the weights of GPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"some test text\", return_tensors='pt')\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "        targetmodel = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)  # Set output_hidden_states to True\n",
    "        outputs = targetmodel(input_ids)\n",
    "        hidden_states = outputs.hidden_states  # Output has shape [num_layers, batch_size, sequence_length, hidden_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 39, 768]),\n",
       " tensor([[[ 1.9051e+01,  3.1598e+01, -1.2699e+02,  ...,  1.9869e+01,\n",
       "           -9.6727e+01, -6.3696e+01],\n",
       "          [ 2.0594e+01, -3.3401e+01, -5.5558e+01,  ..., -4.2343e+00,\n",
       "           -4.8823e+01, -2.8938e+01],\n",
       "          [ 1.7966e+01, -5.3368e+01, -0.0000e+00,  ..., -2.2162e+00,\n",
       "            1.6913e+01, -9.6692e-02]]], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_hidden_states = torch.cat(hidden_states, dim=1)\n",
    "res = testblock.crossattention(torch.randn(1, 3, 768), encoder_hidden_states=encoded_hidden_states)\n",
    "encoded_hidden_states.shape, res[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, implementing the forward pass of testblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[  39.2682,  -31.1375,  -91.9388,  ...,   80.6915,   12.0526,\n",
       "             -0.8885],\n",
       "          [  18.6787,   69.9323,  -10.5715,  ...,  113.7082, -143.0773,\n",
       "            -23.7116],\n",
       "          [  68.0760,  -83.1047, -109.8782,  ...,    1.6196, -132.1950,\n",
       "            -10.8186],\n",
       "          ...,\n",
       "          [  47.5304,  -38.1824, -139.6781,  ...,    3.6882, -129.7056,\n",
       "            -36.8067],\n",
       "          [  45.1635,  -30.1209, -150.8162,  ...,  114.6934,   16.1154,\n",
       "            -29.9821],\n",
       "          [  37.3090,  -44.7251, -123.3011,  ...,  100.8260, -107.3991,\n",
       "            -25.8370]]], grad_fn=<AddBackward0>),)"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testblock.forward(editor_hidden_states[0], encoder_hidden_states=encoded_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seems like it worked!? Now let's make a function to generically input a layer (like testnetwork) and output the modified layer\n",
    "def add_cross_attention_to_layer(model,layer_idx):\n",
    "    block = model.h[layer_idx]\n",
    "    block.crossattention = GPT2Attention(model.config, is_cross_attention=True)\n",
    "    block.ln_cross_attn = nn.LayerNorm(normalized_shape = 768, eps=model.config.layer_norm_epsilon)\n",
    "    original_query_weights = block.attn.c_attn.weight[:,:768]\n",
    "    original_keys_values = block.attn.c_attn.weight[:,768:]\n",
    "    original_query_bias = block.attn.c_attn.bias[:768]\n",
    "    original_keys_values_bias = block.attn.c_attn.bias[768:]\n",
    "    with torch.no_grad():\n",
    "        # Initialize the new layer with these parameters\n",
    "        block.crossattention.q_attn.weight = nn.Parameter(original_query_weights)\n",
    "        block.crossattention.q_attn.bias =  nn.Parameter(original_query_bias)\n",
    "        block.crossattention.c_attn.weight = nn.Parameter(original_keys_values)\n",
    "        block.crossattention.c_attn.bias = nn.Parameter(original_keys_values_bias)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    add_cross_attention_to_layer(model,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup to test the forward pass\n",
    "edit_instructions = \"replace dog with cat\" #Note to self: it's a good idea, for purposes of auto-editing, to look into how hard it is to get the editor to literally replace one set of tokens with another; since the encoding is literally the \"correct\" optimal solution in this case, so we can directly check if it's working on a task where the edit is equivalent to token replacement. Will tell us how many samples are necessary, can it possible work, etc.\n",
    "editor_ids = tokenizer.encode(edit_instructions, return_tensors=\"pt\")\n",
    "\n",
    "target_ids = tokenizer.encode(\"the quick brown fox jumped over the lazy cat\", return_tensors='pt')\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "        targetmodel = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)  # Set output_hidden_states to True\n",
    "        outputs = targetmodel(input_ids)\n",
    "        hidden_states = outputs.hidden_states  # Output has shape [num_layers, batch_size, sequence_length, hidden_size]\n",
    "\n",
    "\n",
    "encoded_hidden_states = torch.cat(hidden_states, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 768])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, let's try to run the forward pass\n",
    "res = model(editor_ids, encoder_hidden_states=encoded_hidden_states)\n",
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(\"some test text\", return_tensors='pt')\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "        targetmodel = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)  # Set output_hidden_states to True\n",
    "        outputs = targetmodel(input_ids)\n",
    "        hidden_states = outputs.hidden_states  # Output has shape [num_layers, batch_size, sequence_length, hidden_size]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# A raw (needs to be fixed) looping funciton\n",
    "        for module_name in self.target_config[\"target_layers\"]:\n",
    "            #path = parse_module_path(module_name)\n",
    "            #module = get_module(model, path)\n",
    "            # Apply your replacement function here\n",
    "            #new_module = replace_layer(module, BiasEditedTransformerLayer, bias = torch.zeros(model.config.n_embd))\n",
    "            #set_module(model, path, new_module)\n",
    "            module = self.model.transformer.h[module_name]\n",
    "            new_module = replace_layer(module, BiasEditedTransformerLayer, bias = torch.zeros(self.target_config[\"target_n_embed\"]))\n",
    "            self.model.transformer.h[module_name] = new_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.add_cross_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    @add_start_docstrings_to_model_forward(GPT2_INPUTS_DOCSTRING)\n",
      "    @add_code_sample_docstrings(\n",
      "        checkpoint=_CHECKPOINT_FOR_DOC,\n",
      "        output_type=BaseModelOutputWithPastAndCrossAttentions,\n",
      "        config_class=_CONFIG_FOR_DOC,\n",
      "    )\n",
      "    def forward(\n",
      "        self,\n",
      "        input_ids: Optional[torch.LongTensor] = None,\n",
      "        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n",
      "        attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        token_type_ids: Optional[torch.LongTensor] = None,\n",
      "        position_ids: Optional[torch.LongTensor] = None,\n",
      "        head_mask: Optional[torch.FloatTensor] = None,\n",
      "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
      "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
      "        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n",
      "        use_cache: Optional[bool] = None,\n",
      "        output_attentions: Optional[bool] = None,\n",
      "        output_hidden_states: Optional[bool] = None,\n",
      "        return_dict: Optional[bool] = None,\n",
      "    ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n",
      "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
      "        output_hidden_states = (\n",
      "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
      "        )\n",
      "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
      "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
      "\n",
      "        if input_ids is not None and inputs_embeds is not None:\n",
      "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
      "        elif input_ids is not None:\n",
      "            input_shape = input_ids.size()\n",
      "            input_ids = input_ids.view(-1, input_shape[-1])\n",
      "            batch_size = input_ids.shape[0]\n",
      "        elif inputs_embeds is not None:\n",
      "            input_shape = inputs_embeds.size()[:-1]\n",
      "            batch_size = inputs_embeds.shape[0]\n",
      "        else:\n",
      "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
      "\n",
      "        device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
      "\n",
      "        if token_type_ids is not None:\n",
      "            token_type_ids = token_type_ids.view(-1, input_shape[-1])\n",
      "        if position_ids is not None:\n",
      "            position_ids = position_ids.view(-1, input_shape[-1])\n",
      "\n",
      "        if past_key_values is None:\n",
      "            past_length = 0\n",
      "            past_key_values = tuple([None] * len(self.h))\n",
      "        else:\n",
      "            past_length = past_key_values[0][0].size(-2)\n",
      "        if position_ids is None:\n",
      "            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n",
      "            position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
      "\n",
      "        # GPT2Attention mask.\n",
      "        if attention_mask is not None:\n",
      "            if batch_size <= 0:\n",
      "                raise ValueError(\"batch_size has to be defined and > 0\")\n",
      "            attention_mask = attention_mask.view(batch_size, -1)\n",
      "            # We create a 3D attention mask from a 2D tensor mask.\n",
      "            # Sizes are [batch_size, 1, 1, to_seq_length]\n",
      "            # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
      "            # this attention mask is more simple than the triangular masking of causal attention\n",
      "            # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
      "            attention_mask = attention_mask[:, None, None, :]\n",
      "\n",
      "            # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
      "            # masked positions, this operation will create a tensor which is 0.0 for\n",
      "            # positions we want to attend and the dtype's smallest value for masked positions.\n",
      "            # Since we are adding it to the raw scores before the softmax, this is\n",
      "            # effectively the same as removing these entirely.\n",
      "            attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
      "            attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n",
      "\n",
      "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
      "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
      "        if self.config.add_cross_attention and encoder_hidden_states is not None:\n",
      "            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
      "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
      "            if encoder_attention_mask is None:\n",
      "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
      "            encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
      "        else:\n",
      "            encoder_attention_mask = None\n",
      "\n",
      "        # Prepare head mask if needed\n",
      "        # 1.0 in head_mask indicate we keep the head\n",
      "        # attention_probs has shape bsz x n_heads x N x N\n",
      "        # head_mask has shape n_layer x batch x n_heads x N x N\n",
      "        head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n",
      "\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.wte(input_ids)\n",
      "        position_embeds = self.wpe(position_ids)\n",
      "        hidden_states = inputs_embeds + position_embeds\n",
      "\n",
      "        if token_type_ids is not None:\n",
      "            token_type_embeds = self.wte(token_type_ids)\n",
      "            hidden_states = hidden_states + token_type_embeds\n",
      "\n",
      "        hidden_states = self.drop(hidden_states)\n",
      "\n",
      "        output_shape = input_shape + (hidden_states.size(-1),)\n",
      "\n",
      "        if self.gradient_checkpointing and self.training:\n",
      "            if use_cache:\n",
      "                logger.warning_once(\n",
      "                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
      "                )\n",
      "                use_cache = False\n",
      "\n",
      "        presents = () if use_cache else None\n",
      "        all_self_attentions = () if output_attentions else None\n",
      "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
      "        all_hidden_states = () if output_hidden_states else None\n",
      "        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n",
      "            # Model parallel\n",
      "            if self.model_parallel:\n",
      "                torch.cuda.set_device(hidden_states.device)\n",
      "                # Ensure layer_past is on same device as hidden_states (might not be correct)\n",
      "                if layer_past is not None:\n",
      "                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n",
      "                # Ensure that attention_mask is always on the same device as hidden_states\n",
      "                if attention_mask is not None:\n",
      "                    attention_mask = attention_mask.to(hidden_states.device)\n",
      "                if isinstance(head_mask, torch.Tensor):\n",
      "                    head_mask = head_mask.to(hidden_states.device)\n",
      "            if output_hidden_states:\n",
      "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "            if self.gradient_checkpointing and self.training:\n",
      "\n",
      "                def create_custom_forward(module):\n",
      "                    def custom_forward(*inputs):\n",
      "                        # None for past_key_value\n",
      "                        return module(*inputs, use_cache, output_attentions)\n",
      "\n",
      "                    return custom_forward\n",
      "\n",
      "                outputs = torch.utils.checkpoint.checkpoint(\n",
      "                    create_custom_forward(block),\n",
      "                    hidden_states,\n",
      "                    None,\n",
      "                    attention_mask,\n",
      "                    head_mask[i],\n",
      "                    encoder_hidden_states,\n",
      "                    encoder_attention_mask,\n",
      "                )\n",
      "            else:\n",
      "                outputs = block(\n",
      "                    hidden_states,\n",
      "                    layer_past=layer_past,\n",
      "                    attention_mask=attention_mask,\n",
      "                    head_mask=head_mask[i],\n",
      "                    encoder_hidden_states=encoder_hidden_states,\n",
      "                    encoder_attention_mask=encoder_attention_mask,\n",
      "                    use_cache=use_cache,\n",
      "                    output_attentions=output_attentions,\n",
      "                )\n",
      "\n",
      "            hidden_states = outputs[0]\n",
      "            if use_cache is True:\n",
      "                presents = presents + (outputs[1],)\n",
      "\n",
      "            if output_attentions:\n",
      "                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n",
      "                if self.config.add_cross_attention:\n",
      "                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n",
      "\n",
      "            # Model Parallel: If it's the last layer for that device, put things on the next device\n",
      "            if self.model_parallel:\n",
      "                for k, v in self.device_map.items():\n",
      "                    if i == v[-1] and \"cuda:\" + str(k) != self.last_device:\n",
      "                        hidden_states = hidden_states.to(\"cuda:\" + str(k + 1))\n",
      "\n",
      "        hidden_states = self.ln_f(hidden_states)\n",
      "\n",
      "        hidden_states = hidden_states.view(output_shape)\n",
      "        # Add last hidden state\n",
      "        if output_hidden_states:\n",
      "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\n",
      "        if not return_dict:\n",
      "            return tuple(\n",
      "                v\n",
      "                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n",
      "                if v is not None\n",
      "            )\n",
      "\n",
      "        return BaseModelOutputWithPastAndCrossAttentions(\n",
      "            last_hidden_state=hidden_states,\n",
      "            past_key_values=presents,\n",
      "            hidden_states=all_hidden_states,\n",
      "            attentions=all_self_attentions,\n",
      "            cross_attentions=all_cross_attentions,\n",
      "        )\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Actually, first, let's inspect the model code above the level of the block\n",
    "print(inspect.getsource(GPT2Model.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, implementing this in all layers of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "for layer_idx in range(12):\n",
    "    testblock = model.transformer.h[layer_idx]\n",
    "    testblock.cross_attention = GPT2Attention(model.config, is_cross_attention=True)\n",
    "    original_query_weights = testblock.attn.c_attn.weight[:,:768]\n",
    "    original_keys_values = testblock.attn.c_attn.weight[:,768:]\n",
    "    original_query_bias = testblock.attn.c_attn.bias[:768]\n",
    "    original_keys_values_bias = testblock.attn.c_attn.bias[768:]   \n",
    "    with torch.no_grad():\n",
    "    # Initialize the new layer with these parameters\n",
    "    testblock.cross_attention.q_attn.weight = nn.Parameter(original_query_weights)\n",
    "    testblock.cross_attention.q_attn.bias =  nn.Parameter(original_query_bias)\n",
    "\n",
    "    testblock.cross_attention.c_attn.weight = nn.Parameter(original_keys_values)\n",
    "    testblock.cross_attention.c_attn.bias = nn.Parameter(original_keys_values_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pytorch_utils.Conv1D"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (cross_attention): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (q_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "testblock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /Users/michaelsklar/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/config.json\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /Users/michaelsklar/.cache/huggingface/hub/models--gpt2/snapshots/11c5a3d5811f50298f278a704980280950aedb10/model.safetensors\n",
      "All model checkpoint weights were used when initializing GPT2Model.\n",
      "\n",
      "All the weights of GPT2Model were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2Model for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 9984])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(\"some test text\", return_tensors='pt')\n",
    "with torch.no_grad():  # No need to calculate gradients\n",
    "        model = GPT2Model.from_pretrained('gpt2', output_hidden_states=True)  # Set output_hidden_states to True\n",
    "        outputs = model(input_ids)\n",
    "        hidden_states = outputs.hidden_states  # Output has shape [num_layers, batch_size, sequence_length, hidden_size]\n",
    "print(hidden_states[2].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768])"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stuck here in testing! I'm trying and failing to reproduce the cross-attention pass, when I push thru some data\n",
    "# Perhaps it is connected to the internal error-handling of the attention layer with nones for attention masks?\n",
    "# should ask Ben!\n",
    "# testcrossattnoutput = testblock.cross_attention.q_attn(hidden_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OK, I think that worked! Now I need to figure out how to add this cross-attention layer to every layer in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[278], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhidden_tensor\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (768) must match the existing size (2304) at non-singleton dimension 1.  Target sizes: [3, 768].  Tensor sizes: [2304]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[271], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m testcrossattnoutput \u001b[38;5;241m=\u001b[39m \u001b[43mtestblock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#                 hidden_states,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#                 attention_mask=attention_mask,\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#                 head_mask=head_mask,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#                 output_attentions=output_attentions,\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#             )\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/transformers/models/gpt2/modeling_gpt2.py:309\u001b[0m, in \u001b[0;36mGPT2Attention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mq_attn\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf class is used as cross attention, the weights `q_attn` have to be defined. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m     )\n\u001b[0;32m--> 309\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m key, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_attn(encoder_hidden_states)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_size, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    311\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m encoder_attention_mask\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/lib/python3.9/site-packages/transformers/pytorch_utils.py:103\u001b[0m, in \u001b[0;36mConv1D.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    102\u001b[0m     size_out \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnf,)\n\u001b[0;32m--> 103\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddmm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(size_out)\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (768) must match the existing size (2304) at non-singleton dimension 1.  Target sizes: [3, 768].  Tensor sizes: [2304]"
     ]
    }
   ],
   "source": [
    "testcrossattnoutput = testblock.cross_attention(\n",
    "    hidden_states = hidden_states[0], attention_mask=None, head_mask=None, encoder_hidden_states=hidden_tensor, encoder_attention_mask=None, output_attentions=True)\n",
    "#                 hidden_states,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 head_mask=head_mask,\n",
    "#                 encoder_hidden_states=encoder_hidden_states,\n",
    "#                 encoder_attention_mask=encoder_attention_mask,\n",
    "#                 output_attentions=output_attentions,\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_hidden_states(input_text, model, tokenizer):\n",
    "\n",
    "    # Encode text input to tensor\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "    # Get all hidden states\n",
    "    with torch.no_grad():  # No need to calculate gradients\n",
    "        outputs = model(input_ids)\n",
    "        hidden_states = outputs.hidden_states  # Output has shape [num_layers, batch_size, sequence_length, hidden_size]\n",
    "\n",
    "    return hidden_states\n",
    "testcache = get_all_hidden_states(\"thisisatest\", model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[205], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdor\u001b[49m(testcache)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dor' is not defined"
     ]
    }
   ],
   "source": [
    "dor(testcache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next, we shall have to check if our modified cross-attention actually works!!\n",
    "#First, we need to get some encodings, from the target cache\n",
    "# Then, we need to try feeding it thru running the attention module\n",
    "\n",
    "\n",
    "#If that works, we can try to add it to testblock, and see if the whole block is willing to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, writing the editor model in such a way that it can accept huge inputs from the target model cache.\n",
    "\n",
    "#We can vectorize the residual sterams by collapsing over layers; but it is still tensorized by token position and batch size\n",
    "# The input cache variable has shape (batch_size, sequence_length, cache_size) where cache_size = hidden_size * num_layers in the target network\n",
    "# Although, looking further down, it seems nice to keep the cache in-place so we can later read from it in layer-wise form\n",
    "\n",
    "#We must write the following new nn.Modules:\n",
    "\n",
    "# \"Secondary transformer layer\" which has a residual stream, feedforward, and layernorm as usual [everything same as the base model]\n",
    "#[in practice, this is probably sufficient to just literally copy over everything as a second instance of the base model transformer? \n",
    "# But we must add:\n",
    "\n",
    "#1\n",
    "#2. \"Secondary attention\" which uses MultiHead Attention, queries the base-text editor [IN THE PARALLEL MODEL] ; keys the target cache [TARGET MODEL] ; and values the target cache [TARGET MODEL] ; and outputting to the secondary residual stream in the layer above\n",
    "# Note: this should probably be in addition to, rather than instead-of, the original attention mechanism of gpt-2? That makes things easy, too, since we don't have to delete stuff. \n",
    "# Just need to ensure the whole module has access to the cache, and that we can add this sub-module in the right place.\n",
    "# 3. \"Tertiary attention\" which replaces the output module of the secondary editor. \n",
    "# Each of l (= num_layers_target) tertiary attention heads will\"\n",
    "    # queries the final token and layer of the secondary residual stream [EDITOR MODEL]; \n",
    "    # keys the target cache in a specific layer; and \n",
    "    # values the final token and layer of the secondary residual stream [EDITOR MODEL];\n",
    "# The attentional softmax will be taken simultaneously over all tokens and all layers, incentivizing a single editing intervention bottleneck\n",
    "\n",
    "# 4. Decide what to do with for the secondary editor model's input embeddings. We could literally keep this as a copy of the base model's input embeddings, so that it has some direct text access to the instructions;\n",
    "# but maybe that's not the correct choice. We could decide to delete the embeddings entirely.\n",
    "\n",
    "# 5. Modify the target model class, so that it can accept changes to all layers AND all token positions\n",
    "# 6. Add a penalty for the L-2 norm of the interventions vector [this should be normalized somehow; normalized to the typical norm of activations]\n",
    "\n",
    "# Finally: we should also append a final placeholder token to all text in the editor model! This might be a simple padding change, but we should make sure to implement it in the dataset class.\n",
    "\n",
    "\n",
    "# from transformers import GPT2LMHeadModel\n",
    "# class EditorModelOmniscient(nn.Module):\n",
    "#     # Separating the editor config file, from its base model's configurations\n",
    "#     def __init__(self, editor_yaml_file_path='config/editor_omniscient.yaml'):\n",
    "\n",
    "#         super().__init__()\n",
    "#         # Load the configuration from the YAML file\n",
    "#         with open(editor_yaml_file_path, 'r') as file:\n",
    "#             self.editor_config = yaml.safe_load(file)\n",
    "#         self.basemodel = GPT2LMHeadModel.from_pretrained('gpt2') \n",
    "#         self.secondarymodel = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        \n",
    "\n",
    "\n",
    "#     def forward(self, input_ids, attention_mask=None, target_cache = None):\n",
    "#         basemodel_hiddenstates = self.basemodel(input_ids, attention_mask=attention_mask, output_hidden_states=True).hidden_states\n",
    "        \n",
    "\n",
    "#         #If we want to cut off the model at a certain layer, we can do so here\n",
    "#         hidden_states[self.editor_config[\"cut_layer\"]]\n",
    "        \n",
    "#         else:\n",
    "#             #write code here for using the target cache architecture\n",
    "#             return \n",
    "\n",
    "\n",
    "\n",
    "        #We have a lot more parameters than we need right now, \n",
    "        #but running it thru torch.compile later will cut down waste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = EditorModel()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 768])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "def get_layer_8_residual(text, model, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass through the modified model\n",
    "    with torch.no_grad():\n",
    "        residual_output = model(**inputs)\n",
    "\n",
    "    return residual_output\n",
    "\n",
    "text = \"Your example text here.\"\n",
    "residual_output = get_layer_8_residual(text, model, tokenizer)\n",
    "print(residual_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residual_output[0,-1,:].shape\n",
    "# This will be our actual output.\n",
    "# For now, we are going to pipe it very directly into all tokens of the residual stream of the target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Block(\n",
       "  (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): GPT2Attention(\n",
       "    (c_attn): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "    (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (mlp): GPT2MLP(\n",
       "    (c_fc): Conv1D()\n",
       "    (c_proj): Conv1D()\n",
       "    (act): NewGELUActivation()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.basemodel.transformer.h[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiasEditedTransformerLayer(torch.nn.Module):\n",
    "    def __init__(self, layer, bias):\n",
    "        super(BiasEditedTransformerLayer, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.bias = bias\n",
    "        # Assuming the bias is a fixed tensor; its size depends on the model's hidden size\n",
    "    \n",
    "    def update_bias(self, new_bias):\n",
    "        self.bias = new_bias\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        # Apply the original layer\n",
    "        hidden_states, present = self.layer(x, *args, **kwargs)\n",
    "\n",
    "        # Add the fixed bias to the residual connection\n",
    "        # This assumes that 'hidden_states' is the output you want to modify\n",
    "        hidden_states = hidden_states + self.bias\n",
    "\n",
    "        return hidden_states, present\n",
    "\n",
    "# Example usage:\n",
    "# from transformers import GPT2Model\n",
    "# model = GPT2Model.from_pretrained('gpt2')\n",
    "# model.h[7] = BiasEditedTransformerLayer(model.h[7], bias = torch.zeros(model.config.n_embd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_layer(module, replacement_fn, **kwargs):\n",
    "    return replacement_fn(module, **kwargs)\n",
    "\n",
    "def parse_module_path(path):\n",
    "    \"\"\" Parse the module path into a list of keys \"\"\"\n",
    "    return path.split('.')\n",
    "\n",
    "def get_module(model, path):\n",
    "    \"\"\" Traverse the model to get the module at the specified path \"\"\"\n",
    "    module = model\n",
    "    for key in path:\n",
    "        module = getattr(module, key)\n",
    "    return module\n",
    "\n",
    "def set_module(model, path, new_module):\n",
    "    \"\"\" Set the new module at the specified path in the model \"\"\"\n",
    "    *parent_path, key = path\n",
    "    parent_module = get_module(model, parent_path)\n",
    "    setattr(parent_module, key, new_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetModel(nn.Module):\n",
    "    # Input the target model's configuration file\n",
    "    def __init__(self, target_yaml_file_path='config/target.yaml'):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Load the configuration from the YAML file\n",
    "        #R# Replace gpt2 with loading of a generic model\n",
    "        with open(target_yaml_file_path, 'r') as file:\n",
    "            self.target_config = yaml.safe_load(file)\n",
    "        self.model = GPT2LMHeadModel.from_pretrained('gpt2') \n",
    "        \n",
    "        #Now we iterate over the layers in the list of target layers, and add bias parameters to them\n",
    "        for module_name in self.target_config[\"target_layers\"]:\n",
    "            #path = parse_module_path(module_name)\n",
    "            #module = get_module(model, path)\n",
    "            # Apply your replacement function here\n",
    "            #new_module = replace_layer(module, BiasEditedTransformerLayer, bias = torch.zeros(model.config.n_embd))\n",
    "            #set_module(model, path, new_module)\n",
    "            module = self.model.transformer.h[module_name]\n",
    "            new_module = replace_layer(module, BiasEditedTransformerLayer, bias = torch.zeros(self.target_config[\"target_n_embed\"]))\n",
    "            self.model.transformer.h[module_name] = new_module\n",
    "\n",
    "    # Freeze the parameters of the target model\n",
    "    #def freeze_parameters(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def update_bias(self, new_bias):\n",
    "        #This function only currently accepts a single bias intervention! \n",
    "        for module_name in self.target_config[\"target_layers\"]:\n",
    "            #path = parse_module_path(module_name)\n",
    "            #module = get_module(model, path)\n",
    "            self.model.transformer.h[module_name].update_bias(new_bias) #Currently always the same bias for all layers\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "            \n",
    "\n",
    "#Example usage:\n",
    "            \n",
    "#Initializing the target model and Updating the bias of the target model\n",
    "#target_model = TargetModel()\n",
    "#new_bias = torch.zeros(model.config.n_embd)\n",
    "#target_model.update_bias(new_bias)\n",
    "#target_model.model.transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constructing the wikipedia dataset that we'll use to train the hypernetwork\n",
    "df = pd.read_csv(\"/Users/michaelsklar/aiplay/wikipedia/wikipedia_three_sentences.csv\",nrows = 100)\n",
    "# Define a fixed sequence length\n",
    "def tokenize_and_pad(text, max_length=50):\n",
    "    return tokenizer(text, max_length=max_length, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "df['editor_input_tokenized'] = df['first_sentences'].apply(lambda x: tokenize_and_pad(x))\n",
    "df['target_input_tokenized'] = df['second_sentences'].apply(lambda x: tokenize_and_pad(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['editor_input_tokenized'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HypernetworkEditor(nn.Module):\n",
    "    def __init__(self,editor_yaml_file_path:str,\n",
    "                 target_yaml_file_path:str):\n",
    "        \n",
    "        super().__init__()\n",
    "\n",
    "        #self.config = config #currently does nothing. Keeping as placeholder\n",
    "        with open(editor_yaml_file_path, 'r') as file:\n",
    "            self.editor_config = yaml.safe_load(file)\n",
    "\n",
    "        self.editor_model = EditorModel(editor_yaml_file_path=editor_yaml_file_path)\n",
    "        self.target_model = TargetModel(target_yaml_file_path=target_yaml_file_path)\n",
    "\n",
    "        #malmen had some nonsense here for managing the list of intervention layers\n",
    "        #We are keeping things simple with one one trivial intervention layer for now\n",
    "\n",
    "        self.opt = torch.optim.Adam(\n",
    "            self.editor_model.parameters(),\n",
    "            float(self.editor_config[\"meta_lr\"])\n",
    "        )\n",
    "\n",
    "        self.lossfn = nn.CrossEntropyLoss()\n",
    "\n",
    "    def check_config(self):\n",
    "        #Check that dimensions of the hypernetwork output match the target model's bias width and also matches the config file\n",
    "        return True\n",
    "\n",
    "    def forward(self, editor_inputs, target_inputs, gradients = True):\n",
    "        #Batching not yet implemented! currently works only for a single datapoint at a time. \n",
    "        if gradients:\n",
    "            hypernetwork_output = self.editor_model(editor_inputs[\"input_ids\"], attention_mask=editor_inputs[\"attention_mask\"])\n",
    "            self.target_model.update_bias(hypernetwork_output)\n",
    "            prediction = self.target_model(target_inputs[\"input_ids\"], attention_mask=target_inputs[\"attention_mask\"])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                hypernetwork_output = self.editor_model(editor_inputs[\"input_ids\"], attention_mask=editor_inputs[\"attention_mask\"])\n",
    "                self.target_model.update_bias(hypernetwork_output)\n",
    "                prediction = self.target_model(target_inputs[\"input_ids\"], attention_mask=target_inputs[\"attention_mask\"])\n",
    "        return prediction\n",
    "        \n",
    "    def train(self, dataframe, batch_size = 1, epochs = 1):\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Create a tqdm progress bar\n",
    "            with tqdm(total=len(dataframe), desc=f\"Epoch {epoch + 1}/{epochs}\", unit='batch') as pbar:\n",
    "                for i in range(0, len(dataframe), batch_size):\n",
    "                    self.opt.zero_grad()\n",
    "                    \n",
    "                    # Assuming these lines extract batches correctly from your dataframe\n",
    "                    self.editor_inputs = dataframe['editor_input_tokenized'][i:i+batch_size]\n",
    "                    self.target_inputs = dataframe['target_input_tokenized'][i:i+batch_size] \n",
    "                    self.prediction = self.forward(self.editor_inputs, self.target_inputs, gradients=True)\n",
    "                    self.y = dataframe[\"target_input_tokenized\"][i][\"input_ids\"][:, 1:]\n",
    "                    self.num_tokens = self.prediction.logits.shape[-1]\n",
    "                    self.loss = self.lossfn(self.prediction.logits[:, :-1, :].view(-1, self.num_tokens), self.y.view(-1))\n",
    "                    \n",
    "                    self.loss.backward()\n",
    "                    self.opt.step()\n",
    "\n",
    "                    # Update progress bar\n",
    "                    pbar.update(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperEditor = HypernetworkEditor(editor_yaml_file_path='config/editor.yaml', target_yaml_file_path='config/target.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HyperEditor.prediction.logits[:, :-1, :].view(-1,50257).shape , HyperEditor.y.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/100 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 100/100 [02:24<00:00,  1.45s/batch]\n"
     ]
    }
   ],
   "source": [
    "HyperEditor.train(df, batch_size = 1, epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 50, 50257]), torch.Size([1, 50, 768]))"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_target = HyperEditor.target_model(df['target_input_tokenized'][0][\"input_ids\"], attention_mask=df['target_input_tokenized'][0][\"attention_mask\"])\n",
    "output_editor = HyperEditor.editor_model(df['editor_input_tokenized'][0][\"input_ids\"], attention_mask=df['editor_input_tokenized'][0][\"attention_mask\"])\n",
    "output_target.logits.shape, output_editor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HypernetworkEditor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mHypernetworkEditor\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'HypernetworkEditor' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>first_sentences</th>\n",
       "      <th>second_sentences</th>\n",
       "      <th>third_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anarchism</td>\n",
       "      <td>Anarchism is a political philosophy and moveme...</td>\n",
       "      <td>Anarchism calls for the abolition of the state...</td>\n",
       "      <td>As a historically left-wing movement, placed o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Autism</td>\n",
       "      <td>Autism is a neurodevelopmental disorder charac...</td>\n",
       "      <td>Parents often notice signs during the first th...</td>\n",
       "      <td>These signs often develop gradually, though so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>A, or a, is the first letter and the first vow...</td>\n",
       "      <td>Its name in English is a (pronounced ), plural...</td>\n",
       "      <td>It is similar in shape to the Ancient Greek le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>Alabama () is a state in the Southeastern regi...</td>\n",
       "      <td>Alabama is the 30th largest by area and the 24...</td>\n",
       "      <td>states.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>Abraham Lincoln (; February 12, 1809 – April 1...</td>\n",
       "      <td>Lincoln led the nation through the American Ci...</td>\n",
       "      <td>economy..</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             title                                    first_sentences  \\\n",
       "0        Anarchism  Anarchism is a political philosophy and moveme...   \n",
       "1           Autism  Autism is a neurodevelopmental disorder charac...   \n",
       "2                A  A, or a, is the first letter and the first vow...   \n",
       "3          Alabama  Alabama () is a state in the Southeastern regi...   \n",
       "4  Abraham Lincoln  Abraham Lincoln (; February 12, 1809 – April 1...   \n",
       "\n",
       "                                    second_sentences  \\\n",
       "0  Anarchism calls for the abolition of the state...   \n",
       "1  Parents often notice signs during the first th...   \n",
       "2  Its name in English is a (pronounced ), plural...   \n",
       "3  Alabama is the 30th largest by area and the 24...   \n",
       "4  Lincoln led the nation through the American Ci...   \n",
       "\n",
       "                                     third_sentences  \n",
       "0  As a historically left-wing movement, placed o...  \n",
       "1  These signs often develop gradually, though so...  \n",
       "2  It is similar in shape to the Ancient Greek le...  \n",
       "3                                            states.  \n",
       "4                                          economy..  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class RunningMeanStd(nn.Module):\n",
    "\n",
    "    def __init__(self, size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"n\", torch.zeros(1))\n",
    "        self.register_buffer(\"mean\", torch.zeros((size)))\n",
    "        self.register_buffer(\"var\", torch.zeros((size)))\n",
    "        self.register_buffer(\"std\", torch.zeros((size)))\n",
    "\n",
    "    def update(self, x: torch.FloatTensor):\n",
    "\n",
    "        n = self.n + x.shape[0]\n",
    "        delta = x.mean(0) - self.mean\n",
    "        self.mean += x.shape[0] * delta / n\n",
    "        self.var += x.shape[0] * x.var(0) + self.n * x.shape[0] * delta.pow(2) / n\n",
    "        self.std = (self.var / (n - 1 + torch.finfo(x.dtype).eps)).sqrt()\n",
    "        self.n = n\n",
    "              \n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "\n",
    "        return (x - self.mean) / (self.std + torch.finfo(x.dtype).eps)\n",
    "\n",
    "\n",
    "class MALMENBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, size: int, rank: int, n_modules: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.A = nn.Parameter(torch.randn(size, rank))\n",
    "        self.B = nn.Parameter(torch.zeros(rank, size))\n",
    "        self.bias = nn.Parameter(torch.zeros(size))\n",
    "        \n",
    "        self.scale = nn.Embedding(n_modules, size)\n",
    "        self.shift = nn.Embedding(n_modules, size)\n",
    "        \n",
    "        self.scale.weight.data.fill_(1)\n",
    "        self.shift.weight.data.fill_(0)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        y: torch.FloatTensor,\n",
    "        module_idx: torch.LongTensor\n",
    "    ) -> torch.FloatTensor:\n",
    "\n",
    "        x = y @ self.A @ self.B + self.bias\n",
    "        x = x.clamp(0)\n",
    "        x = self.scale(module_idx) * x + self.shift(module_idx)\n",
    "        x = x + y\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MALMENNet(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key_size: int,\n",
    "        value_size: int,\n",
    "        rank: int,\n",
    "        n_blocks: int,\n",
    "        n_modules: int,\n",
    "        lr: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.key_size = key_size\n",
    "        self.value_size = value_size\n",
    "\n",
    "        self.normalizer = RunningMeanStd(key_size + value_size)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MALMENBlock(key_size + value_size, rank, n_modules)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "        self.lr = nn.Embedding(n_modules, 1)\n",
    "        self.lamda = nn.Embedding(n_modules, 1)\n",
    "        \n",
    "        self.lr.weight.data.fill_(lr)\n",
    "        self.lamda.weight.data.fill_(0)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        keys: torch.FloatTensor,\n",
    "        values_grad: torch.FloatTensor,\n",
    "        module_idx: torch.LongTensor\n",
    "    ) -> Tuple[torch.FloatTensor]:\n",
    "\n",
    "        hidden_states = torch.cat((keys, values_grad), -1)\n",
    "        hidden_states = self.normalizer(hidden_states)\n",
    "        for block in self.blocks:\n",
    "            hidden_states = block(hidden_states, module_idx)\n",
    "        return hidden_states.split([self.key_size, self.value_size], -1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
